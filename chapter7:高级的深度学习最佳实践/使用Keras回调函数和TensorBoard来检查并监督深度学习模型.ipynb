{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程中将回调函数作用于模型\n",
    "--------\n",
    "    * 模型检查点\n",
    "    * 提前终止\n",
    "    * 在训练过程中动态调节某些参数值\n",
    "    * 在训练过程中记录训练指标和验证指标,或将模型学到的表示可视化(这些表示也在不断更新)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint 与 EarlyStopping 回调函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "callbacks_list = [\n",
    "    # 如果不在改善,就中断训练\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',  # 监控模型的验证精度\n",
    "        patience=1,  # 如果精度在多于一轮的时间(即两轮)内不再改善,中断训练\n",
    "    ),\n",
    "    # 在每轮过后保存当前权重\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='my_model.h5',  # 目标模型文件的保存路径\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True  # 如果val_loss没有改善,那么不需要覆盖模型文件.这就可以始终保存在训练过程中见到的最佳模型\n",
    "    )\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rsmprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceLROnPlateau 回调函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',  # 监控模型的验证损失\n",
    "        factor=0.1,  # 触发时将学习率除以10\n",
    "        patience=10  # 如果验证损失在10轮内没有改善,那么就触发这个回调函数\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编写自己的回调函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在每轮次结束后将模型每层的激活保存到硬盘(格式为Numpy数组),这个激活是对验证集的第一个样本计算得到的\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        self.activations_model = keras.modles.Model(model.input, layer_outputs)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data')\n",
    "        validation_sample = self.validation_data[0][0:1]    \n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        f = open('activations_at_epoch_'+str(epoch)+'.npz', 'w')\n",
    "        np.savez(f, activations)\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow的可视化框架:TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用了TensorBoard的文本分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 2000\n",
    "max_len = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len, name='embed'))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为TensorBoard日志文件创建一个目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir my_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34701,\n",
       " 52006,\n",
       " 52007,\n",
       " 16816,\n",
       " 63951,\n",
       " 1408,\n",
       " 16115,\n",
       " 2345,\n",
       " 2289,\n",
       " 52008,\n",
       " 52009,\n",
       " 11307,\n",
       " 40830,\n",
       " 30568,\n",
       " 52010,\n",
       " 40831,\n",
       " 52011,\n",
       " 19313,\n",
       " 52012,\n",
       " 52013,\n",
       " 25242,\n",
       " 6746,\n",
       " 52014,\n",
       " 52015,\n",
       " 52016,\n",
       " 68804,\n",
       " 52017,\n",
       " 40833,\n",
       " 34702,\n",
       " 2338,\n",
       " 40834,\n",
       " 34703,\n",
       " 52018,\n",
       " 16817,\n",
       " 1636,\n",
       " 16818,\n",
       " 52019,\n",
       " 34704,\n",
       " 52020,\n",
       " 11585,\n",
       " 57766,\n",
       " 52021,\n",
       " 14129,\n",
       " 52023,\n",
       " 11038,\n",
       " 52025,\n",
       " 29114,\n",
       " 52027,\n",
       " 52125,\n",
       " 40835,\n",
       " 52028,\n",
       " 52130,\n",
       " 34706,\n",
       " 27631,\n",
       " 40836,\n",
       " 15492,\n",
       " 52030,\n",
       " 11926,\n",
       " 4010,\n",
       " 3230,\n",
       " 52031,\n",
       " 34707,\n",
       " 30585,\n",
       " 52033,\n",
       " 40837,\n",
       " 26338,\n",
       " 52034,\n",
       " 30569,\n",
       " 52035,\n",
       " 52036,\n",
       " 40839,\n",
       " 52037,\n",
       " 52038,\n",
       " 11927,\n",
       " 16819,\n",
       " 52039,\n",
       " 25243,\n",
       " 21905,\n",
       " 52040,\n",
       " 40840,\n",
       " 40841,\n",
       " 359,\n",
       " 5034,\n",
       " 7093,\n",
       " 21906,\n",
       " 23379,\n",
       " 52041,\n",
       " 52042,\n",
       " 18510,\n",
       " 30570,\n",
       " 9878,\n",
       " 40842,\n",
       " 52043,\n",
       " 52044,\n",
       " 52045,\n",
       " 40843,\n",
       " 34708,\n",
       " 25244,\n",
       " 7180,\n",
       " 52046,\n",
       " 40844,\n",
       " 11586,\n",
       " 20598,\n",
       " 52047,\n",
       " 11037,\n",
       " 52048,\n",
       " 52049,\n",
       " 52050,\n",
       " 17633,\n",
       " 52051,\n",
       " 30602,\n",
       " 40846,\n",
       " 52052,\n",
       " 52053,\n",
       " 23380,\n",
       " 52054,\n",
       " 27633,\n",
       " 52055,\n",
       " 10307,\n",
       " 52057,\n",
       " 42577,\n",
       " 15493,\n",
       " 40847,\n",
       " 52058,\n",
       " 22922,\n",
       " 52059,\n",
       " 27634,\n",
       " 5621,\n",
       " 40849,\n",
       " 52060,\n",
       " 52061,\n",
       " 16821,\n",
       " 40850,\n",
       " 52062,\n",
       " 40851,\n",
       " 19500,\n",
       " 34710,\n",
       " 52063,\n",
       " 5283,\n",
       " 30571,\n",
       " 25245,\n",
       " 52321,\n",
       " 34711,\n",
       " 40852,\n",
       " 178,\n",
       " 30572,\n",
       " 52065,\n",
       " 21908,\n",
       " 2103,\n",
       " 52066,\n",
       " 11928,\n",
       " 22433,\n",
       " 52067,\n",
       " 6702,\n",
       " 30573,\n",
       " 52068,\n",
       " 27635,\n",
       " 16116,\n",
       " 52069,\n",
       " 3930,\n",
       " 352,\n",
       " 52070,\n",
       " 52071,\n",
       " 34712,\n",
       " 40853,\n",
       " 40854,\n",
       " 40855,\n",
       " 27636,\n",
       " 52072,\n",
       " 4760,\n",
       " 52073,\n",
       " 23381,\n",
       " 34713,\n",
       " 27637,\n",
       " 40856,\n",
       " 9092,\n",
       " 30071,\n",
       " 52075,\n",
       " 23382,\n",
       " 14896,\n",
       " 52077,\n",
       " 6703,\n",
       " 40857,\n",
       " 30623,\n",
       " 20599,\n",
       " 11308,\n",
       " 1180,\n",
       " 52078,\n",
       " 40859,\n",
       " 1927,\n",
       " 4287,\n",
       " 52079,\n",
       " 52080,\n",
       " 52081,\n",
       " 34714,\n",
       " 52082,\n",
       " 52083,\n",
       " 52084,\n",
       " 30574,\n",
       " 52085,\n",
       " 52086,\n",
       " 12606,\n",
       " 31931,\n",
       " 52088,\n",
       " 52089,\n",
       " 13885,\n",
       " 34715,\n",
       " 9654,\n",
       " 299,\n",
       " 52090,\n",
       " 52091,\n",
       " 52092,\n",
       " 52093,\n",
       " 27638,\n",
       " 52094,\n",
       " 8775,\n",
       " 21909,\n",
       " 52095,\n",
       " 6192,\n",
       " 30575,\n",
       " 52096,\n",
       " 52097,\n",
       " 52098,\n",
       " 52099,\n",
       " 17634,\n",
       " 52100,\n",
       " 52101,\n",
       " 52102,\n",
       " 17635,\n",
       " 40860,\n",
       " 23383,\n",
       " 57781,\n",
       " 52104,\n",
       " 7916,\n",
       " 30576,\n",
       " 48246,\n",
       " 10553,\n",
       " 40861,\n",
       " 17649,\n",
       " 40863,\n",
       " 52105,\n",
       " 52106,\n",
       " 34716,\n",
       " 52107,\n",
       " 30577,\n",
       " 40864,\n",
       " 11309,\n",
       " 52108,\n",
       " 40865,\n",
       " 16117,\n",
       " 40866,\n",
       " 4011,\n",
       " 40867,\n",
       " 52109,\n",
       " 82143,\n",
       " 37519,\n",
       " 1285,\n",
       " 52112,\n",
       " 28105,\n",
       " 52113,\n",
       " 52114,\n",
       " 18511,\n",
       " 30578,\n",
       " 52115,\n",
       " 46449,\n",
       " 52117,\n",
       " 34718,\n",
       " 6792,\n",
       " 52118,\n",
       " 27639,\n",
       " 27640,\n",
       " 52119,\n",
       " 52024,\n",
       " 884,\n",
       " 34719,\n",
       " 1297,\n",
       " 25905,\n",
       " 40868,\n",
       " 52120,\n",
       " 52121,\n",
       " 14897,\n",
       " 12607,\n",
       " 30579,\n",
       " 40869,\n",
       " 52122,\n",
       " 11039,\n",
       " 52123,\n",
       " 52124,\n",
       " 5399,\n",
       " 34705,\n",
       " 34720,\n",
       " 9836,\n",
       " 13886,\n",
       " 52127,\n",
       " 30580,\n",
       " 52128,\n",
       " 34721,\n",
       " 52129,\n",
       " 52029,\n",
       " 544,\n",
       " 52131,\n",
       " 33266,\n",
       " 52133,\n",
       " 52134,\n",
       " 40870,\n",
       " 40871,\n",
       " 40872,\n",
       " 52135,\n",
       " 52136,\n",
       " 57596,\n",
       " 5622,\n",
       " 7799,\n",
       " 52137,\n",
       " 25246,\n",
       " 52138,\n",
       " 52139,\n",
       " 52140,\n",
       " 52141,\n",
       " 40873,\n",
       " 52142,\n",
       " 30581,\n",
       " 52143,\n",
       " 30582,\n",
       " 18512,\n",
       " 1357,\n",
       " 14898,\n",
       " 40874,\n",
       " 40875,\n",
       " 40876,\n",
       " 16822,\n",
       " 19502,\n",
       " 34722,\n",
       " 52144,\n",
       " 27641,\n",
       " 52145,\n",
       " 21910,\n",
       " 52146,\n",
       " 39797,\n",
       " 35731,\n",
       " 40879,\n",
       " 52147,\n",
       " 4988,\n",
       " 32018,\n",
       " 30583,\n",
       " 76125,\n",
       " 40881,\n",
       " 52149,\n",
       " 657,\n",
       " 41095,\n",
       " 23418,\n",
       " 27642,\n",
       " 52152,\n",
       " 52153,\n",
       " 8422,\n",
       " 57792,\n",
       " 40882,\n",
       " 52154,\n",
       " 5745,\n",
       " 34724,\n",
       " 25247,\n",
       " 52155,\n",
       " 15494,\n",
       " 30584,\n",
       " 17177,\n",
       " 52156,\n",
       " 52032,\n",
       " 34725,\n",
       " 21911,\n",
       " 52157,\n",
       " 7476,\n",
       " 40883,\n",
       " 40884,\n",
       " 40885,\n",
       " 20600,\n",
       " 34726,\n",
       " 40886,\n",
       " 52159,\n",
       " 27643,\n",
       " 3899,\n",
       " 42584,\n",
       " 52160,\n",
       " 40887,\n",
       " 14360,\n",
       " 8776,\n",
       " 21912,\n",
       " 34727,\n",
       " 1777,\n",
       " 4588,\n",
       " 18513,\n",
       " 10086,\n",
       " 50202,\n",
       " 52161,\n",
       " 27644,\n",
       " 52162,\n",
       " 40888,\n",
       " 47360,\n",
       " 52163,\n",
       " 52164,\n",
       " 40889,\n",
       " 40890,\n",
       " 41128,\n",
       " 40891,\n",
       " 17636,\n",
       " 52166,\n",
       " 772,\n",
       " 9259,\n",
       " 9879,\n",
       " 52167,\n",
       " 40892,\n",
       " 34728,\n",
       " 52168,\n",
       " 6887,\n",
       " 21913,\n",
       " 52169,\n",
       " 27645,\n",
       " 40893,\n",
       " 7693,\n",
       " 52961,\n",
       " 11310,\n",
       " 873,\n",
       " 52170,\n",
       " 40894,\n",
       " 30586,\n",
       " 3900,\n",
       " 52171,\n",
       " 2823,\n",
       " 52172,\n",
       " 52173,\n",
       " 52174,\n",
       " 52175,\n",
       " 20601,\n",
       " 52176,\n",
       " 40895,\n",
       " 52177,\n",
       " 40896,\n",
       " 52178,\n",
       " 30587,\n",
       " 40898,\n",
       " 52179,\n",
       " 34870,\n",
       " 52181,\n",
       " 40899,\n",
       " 35714,\n",
       " 40901,\n",
       " 52182,\n",
       " 27646,\n",
       " 52183,\n",
       " 52184,\n",
       " 31268,\n",
       " 11929,\n",
       " 40902,\n",
       " 21914,\n",
       " 30588,\n",
       " 40903,\n",
       " 52185,\n",
       " 25248,\n",
       " 8777,\n",
       " 52186,\n",
       " 73,\n",
       " 34730,\n",
       " 22615,\n",
       " 27648,\n",
       " 40904,\n",
       " 52187,\n",
       " 11587,\n",
       " 40905,\n",
       " 40906,\n",
       " 14361,\n",
       " 30589,\n",
       " 40907,\n",
       " 6259,\n",
       " 7364,\n",
       " 21915,\n",
       " 16823,\n",
       " 21916,\n",
       " 4989,\n",
       " 3084,\n",
       " 53083,\n",
       " 27649,\n",
       " 10584,\n",
       " 52191,\n",
       " 18514,\n",
       " 40908,\n",
       " 52192,\n",
       " 52193,\n",
       " 40909,\n",
       " 16479,\n",
       " 34731,\n",
       " 52195,\n",
       " 57804,\n",
       " 40910,\n",
       " 14899,\n",
       " 40911,\n",
       " 52196,\n",
       " 41188,\n",
       " 34732,\n",
       " 53153,\n",
       " 2576,\n",
       " 52198,\n",
       " 30592,\n",
       " 25249,\n",
       " 20638,\n",
       " 52200,\n",
       " 19504,\n",
       " 27650,\n",
       " 19505,\n",
       " 40912,\n",
       " 52201,\n",
       " 25250,\n",
       " 23385,\n",
       " 40913,\n",
       " 34733,\n",
       " 27651,\n",
       " 52202,\n",
       " 52203,\n",
       " 34734,\n",
       " 69876,\n",
       " 52204,\n",
       " 34735,\n",
       " 52205,\n",
       " 40914,\n",
       " 9094,\n",
       " 20602,\n",
       " 40915,\n",
       " 52206,\n",
       " 52207,\n",
       " 52208,\n",
       " 40916,\n",
       " 4449,\n",
       " 40917,\n",
       " 30593,\n",
       " 19506,\n",
       " 53241,\n",
       " 40918,\n",
       " 75638,\n",
       " 52209,\n",
       " 52210,\n",
       " 33946,\n",
       " 52211,\n",
       " 19023,\n",
       " 20603,\n",
       " 13420,\n",
       " 34736,\n",
       " 25251,\n",
       " 4311,\n",
       " 4892,\n",
       " 53259,\n",
       " 52213,\n",
       " 66,\n",
       " 11930,\n",
       " 16825,\n",
       " 25252,\n",
       " 13421,\n",
       " 20604,\n",
       " 52214,\n",
       " 30731,\n",
       " 30594,\n",
       " 13887,\n",
       " 44,\n",
       " 2401,\n",
       " 40919,\n",
       " 30595,\n",
       " 52215,\n",
       " 15495,\n",
       " 52216,\n",
       " 52217,\n",
       " 27652,\n",
       " 23673,\n",
       " 52219,\n",
       " 12997,\n",
       " 40920,\n",
       " 52220,\n",
       " 52221,\n",
       " 27653,\n",
       " 32622,\n",
       " 52222,\n",
       " 52223,\n",
       " 34935,\n",
       " 52225,\n",
       " 34737,\n",
       " 52226,\n",
       " 39724,\n",
       " 30597,\n",
       " 40401,\n",
       " 2290,\n",
       " 10087,\n",
       " 56434,\n",
       " 34738,\n",
       " 6627,\n",
       " 52227,\n",
       " 27654,\n",
       " 53344,\n",
       " 52229,\n",
       " 52230,\n",
       " 40923,\n",
       " 52231,\n",
       " 52232,\n",
       " 40924,\n",
       " 1321,\n",
       " 53363,\n",
       " 52233,\n",
       " 13888,\n",
       " 52234,\n",
       " 52235,\n",
       " 30598,\n",
       " 40925,\n",
       " 52236,\n",
       " 51995,\n",
       " 40926,\n",
       " 1181,\n",
       " 52237,\n",
       " 52238,\n",
       " 52239,\n",
       " 52240,\n",
       " 52241,\n",
       " 52242,\n",
       " 17637,\n",
       " 40927,\n",
       " 27655,\n",
       " 52243,\n",
       " 21962,\n",
       " 30599,\n",
       " 30600,\n",
       " 52244,\n",
       " 52245,\n",
       " 25255,\n",
       " 5340,\n",
       " 9095,\n",
       " 3231,\n",
       " 12258,\n",
       " 52246,\n",
       " 8289,\n",
       " 18515,\n",
       " 3537,\n",
       " 52247,\n",
       " 17638,\n",
       " 18516,\n",
       " 40928,\n",
       " 8926,\n",
       " 52248,\n",
       " 27656,\n",
       " 52249,\n",
       " 23386,\n",
       " 40929,\n",
       " 21918,\n",
       " 52250,\n",
       " 52251,\n",
       " 34740,\n",
       " 40930,\n",
       " 49759,\n",
       " 52253,\n",
       " 30601,\n",
       " 34741,\n",
       " 52254,\n",
       " 34742,\n",
       " 10308,\n",
       " 52255,\n",
       " 5924,\n",
       " 40845,\n",
       " 52256,\n",
       " 59386,\n",
       " 52257,\n",
       " 7253,\n",
       " 25256,\n",
       " 52259,\n",
       " 9457,\n",
       " 6041,\n",
       " 52260,\n",
       " 5120,\n",
       " 52261,\n",
       " 40931,\n",
       " 52262,\n",
       " 52263,\n",
       " 14900,\n",
       " 52264,\n",
       " 52265,\n",
       " 34743,\n",
       " 9880,\n",
       " 52266,\n",
       " 27657,\n",
       " 40932,\n",
       " 52267,\n",
       " 52268,\n",
       " 52269,\n",
       " 52270,\n",
       " 57816,\n",
       " 20606,\n",
       " 52271,\n",
       " 40933,\n",
       " 21919,\n",
       " 40934,\n",
       " 64007,\n",
       " 5558,\n",
       " 21920,\n",
       " 12998,\n",
       " 52272,\n",
       " 52273,\n",
       " 3975,\n",
       " 8059,\n",
       " 52274,\n",
       " 6704,\n",
       " 18517,\n",
       " 52275,\n",
       " 40935,\n",
       " 52276,\n",
       " 34744,\n",
       " 52277,\n",
       " 3626,\n",
       " 2497,\n",
       " 52278,\n",
       " 52279,\n",
       " 52280,\n",
       " 3036,\n",
       " 52281,\n",
       " 34745,\n",
       " 4517,\n",
       " 34746,\n",
       " 76152,\n",
       " 30603,\n",
       " 52283,\n",
       " 52284,\n",
       " 45879,\n",
       " 18518,\n",
       " 52285,\n",
       " 40937,\n",
       " 18519,\n",
       " 21921,\n",
       " 5623,\n",
       " 19507,\n",
       " 25257,\n",
       " 11311,\n",
       " 21922,\n",
       " 40938,\n",
       " 18557,\n",
       " 3396,\n",
       " 11931,\n",
       " 34748,\n",
       " 21923,\n",
       " 52286,\n",
       " 21924,\n",
       " 10785,\n",
       " 7365,\n",
       " 52287,\n",
       " 52288,\n",
       " 20607,\n",
       " 2894,\n",
       " 52289,\n",
       " 2588,\n",
       " 52290,\n",
       " 783,\n",
       " 16118,\n",
       " 40940,\n",
       " 34749,\n",
       " 52291,\n",
       " 38765,\n",
       " 52292,\n",
       " 34750,\n",
       " 16826,\n",
       " 30604,\n",
       " 52293,\n",
       " 52294,\n",
       " 40941,\n",
       " 11932,\n",
       " 82190,\n",
       " 11312,\n",
       " 61534,\n",
       " 41342,\n",
       " 40942,\n",
       " 41346,\n",
       " 34751,\n",
       " 52298,\n",
       " 52299,\n",
       " 13422,\n",
       " 6538,\n",
       " 52300,\n",
       " 40943,\n",
       " 31270,\n",
       " 52301,\n",
       " 52302,\n",
       " 52303,\n",
       " 12999,\n",
       " 3383,\n",
       " 14362,\n",
       " 52304,\n",
       " 52305,\n",
       " 18520,\n",
       " 9096,\n",
       " 9655,\n",
       " 34752,\n",
       " 30074,\n",
       " 5514,\n",
       " 40944,\n",
       " 42592,\n",
       " 19508,\n",
       " 9458,\n",
       " 52307,\n",
       " 40945,\n",
       " 21925,\n",
       " 52308,\n",
       " 3840,\n",
       " 37567,\n",
       " 52309,\n",
       " 34753,\n",
       " 52310,\n",
       " 52311,\n",
       " 13423,\n",
       " 25259,\n",
       " 52312,\n",
       " 52313,\n",
       " 40946,\n",
       " 27658,\n",
       " 27659,\n",
       " 19509,\n",
       " 52315,\n",
       " 21926,\n",
       " 37778,\n",
       " 27660,\n",
       " 3627,\n",
       " 52316,\n",
       " 11799,\n",
       " 52317,\n",
       " 34754,\n",
       " 52318,\n",
       " 34755,\n",
       " 40948,\n",
       " 30605,\n",
       " 16827,\n",
       " 52319,\n",
       " 52320,\n",
       " 34756,\n",
       " 40949,\n",
       " 52064,\n",
       " 52322,\n",
       " 52323,\n",
       " 52324,\n",
       " 52325,\n",
       " 21927,\n",
       " 34757,\n",
       " 82196,\n",
       " 1042,\n",
       " 34758,\n",
       " 52326,\n",
       " 52327,\n",
       " 23106,\n",
       " 25260,\n",
       " 40950,\n",
       " 13000,\n",
       " 76164,\n",
       " 52330,\n",
       " 23387,\n",
       " 16019,\n",
       " 34759,\n",
       " 52332,\n",
       " 52333,\n",
       " 27661,\n",
       " 4518,\n",
       " 17639,\n",
       " 54014,\n",
       " 40952,\n",
       " 52336,\n",
       " 52337,\n",
       " 52338,\n",
       " 12259,\n",
       " 4252,\n",
       " 23494,\n",
       " 49580,\n",
       " 52339,\n",
       " 30607,\n",
       " 19510,\n",
       " 34760,\n",
       " 40953,\n",
       " 34761,\n",
       " 52340,\n",
       " 52341,\n",
       " 21928,\n",
       " 52342,\n",
       " 40954,\n",
       " 52343,\n",
       " 40955,\n",
       " 52345,\n",
       " 52346,\n",
       " 14901,\n",
       " 34762,\n",
       " 30608,\n",
       " 40956,\n",
       " 25261,\n",
       " 82201,\n",
       " 40957,\n",
       " 40958,\n",
       " 16828,\n",
       " 40959,\n",
       " 1237,\n",
       " 52348,\n",
       " 52349,\n",
       " 52350,\n",
       " 34763,\n",
       " 52351,\n",
       " 11040,\n",
       " 40960,\n",
       " 20608,\n",
       " 52352,\n",
       " 40961,\n",
       " 34764,\n",
       " 23388,\n",
       " 16120,\n",
       " 18521,\n",
       " 27662,\n",
       " 18522,\n",
       " 52354,\n",
       " 52355,\n",
       " 34765,\n",
       " 52356,\n",
       " 52357,\n",
       " 21929,\n",
       " 52358,\n",
       " 52359,\n",
       " 15496,\n",
       " 52360,\n",
       " 52361,\n",
       " 40962,\n",
       " 25262,\n",
       " 21930,\n",
       " 6539,\n",
       " 12608,\n",
       " 52362,\n",
       " 52363,\n",
       " 52364,\n",
       " 52365,\n",
       " 10786,\n",
       " 41465,\n",
       " 52367,\n",
       " 40963,\n",
       " 57832,\n",
       " 65183,\n",
       " 6260,\n",
       " 76172,\n",
       " 54150,\n",
       " 27663,\n",
       " 40964,\n",
       " 52368,\n",
       " 18523,\n",
       " 30610,\n",
       " 34767,\n",
       " 612,\n",
       " 52369,\n",
       " 52370,\n",
       " 52371,\n",
       " 30611,\n",
       " 52372,\n",
       " 17640,\n",
       " 27664,\n",
       " 7817,\n",
       " 40965,\n",
       " 52374,\n",
       " 35097,\n",
       " 16829,\n",
       " 14902,\n",
       " 7094,\n",
       " 52375,\n",
       " 30612,\n",
       " 52376,\n",
       " 61306,\n",
       " 52377,\n",
       " 6108,\n",
       " 19153,\n",
       " 40966,\n",
       " 52379,\n",
       " 13001,\n",
       " 52380,\n",
       " 21931,\n",
       " 52381,\n",
       " 40967,\n",
       " 52382,\n",
       " 52383,\n",
       " 2847,\n",
       " 34768,\n",
       " 23490,\n",
       " 21932,\n",
       " 40968,\n",
       " 40969,\n",
       " 52385,\n",
       " 25263,\n",
       " 17641,\n",
       " 3249,\n",
       " 87955,\n",
       " 52386,\n",
       " 8424,\n",
       " 41511,\n",
       " 34769,\n",
       " 34770,\n",
       " 28624,\n",
       " 9097,\n",
       " 9260,\n",
       " 25264,\n",
       " 40970,\n",
       " 64027,\n",
       " 19511,\n",
       " 16121,\n",
       " 52389,\n",
       " 52390,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index_dict = imdb.get_word_index()\n",
    "reverse_word_index_list = [v for k,v in word_index_dict.items()]\n",
    "reverse_word_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用一个TensorBoard回调函数来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 0.2465 - acc: 0.9004 - val_loss: 0.3172 - val_acc: 0.8684\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "To visualize embeddings, embeddings_data must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-afc455b30f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    913\u001b[0m                              \"provided, and cannot be a generator.\")\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             raise ValueError(\"To visualize embeddings, embeddings_data must \"\n\u001b[0m\u001b[1;32m    916\u001b[0m                              \"be provided.\")\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: To visualize embeddings, embeddings_data must be provided."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_layer_names = set(layer.name for layer in model.layers if layer.name.startswith('dense_'))\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='my_log_dir/',  # 日志文件将被写入这个位置\n",
    "        histogram_freq=1,  # 每一轮之后记录激活直方图\n",
    "        embeddings_freq=1,  # 每一轮之后记录嵌入数据\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20, \n",
    "    batch_size=128, \n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "plot_model(model, show_shapes=True, to_file='model1.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
